apiVersion: template.openshift.io/v1
kind: Template
metadata:
  annotations:
    description: HF_TGI serving model.
    opendatahub.io/apiProtocol: REST
    opendatahub.io/modelServingSupport: '["single"]'
    openshift.io/provider-display-name: Openshift AI BU.
    tags: rhods,rhoai,kserve,servingruntime
    template.openshift.io/long-description: This template defines resources needed
      to deploy hf_tgi servingruntime with Red Hat Data Science KServe
      for LLM model
  labels:
    app: odh-dashboard
    app.kubernetes.io/part-of: rhods-dashboard
    app.opendatahub.io/rhods-dashboard: "true"
    opendatahub.io/dashboard: "true"
    opendatahub.io/ootb: "true"
  name: hf-tgi-serving-template
  namespace: redhat-ods-applications
objects:
- apiVersion: serving.kserve.io/v1alpha1
  kind: ServingRuntime
  metadata:
    name: hf-tgi-runtime
  spec:
    containers:
      - name: kserve-container
        image: ghcr.io/huggingface/text-generation-inference:1.4.0
        command: ["text-generation-launcher"]
        args:
          - "--model-id=/mnt/models/"
          - "--port=3000"
        env:
        - name: HF_HOME
          value: /tmp/hf_home
        - name: HUGGINGFACE_HUB_CACHE
          value: /tmp/hf_hub_cache
        - name: TRANSFORMER_CACHE
          value: /tmp/transformers_cache
        #resources: # configure as required
        #  requests:
        #    nvidia.com/gpu: 1
        #  limits:
        #    nvidia.com/gpu: 1
        readinessProbe: # Use exec probes instad of httpGet since the probes' port gets rewritten to the containerPort
          exec:
            command:
              - curl
              - localhost:3000/health
          initialDelaySeconds: 30
        livenessProbe:
          exec:
            command:
              - curl
              - localhost:3000/health
          initialDelaySeconds: 30
        ports:
          - containerPort: 3000
            protocol: TCP
    multiModel: false
    supportedModelFormats:
      - autoSelect: true
        name: pytorch