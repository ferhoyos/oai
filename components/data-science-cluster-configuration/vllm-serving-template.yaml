apiVersion: template.openshift.io/v1
kind: Template
metadata:
  annotations:
    description: Vllm serving model.
    opendatahub.io/apiProtocol: REST
    opendatahub.io/modelServingSupport: '["single"]'
    openshift.io/provider-display-name: Openshift AI BU.
    tags: rhods,rhoai,kserve,servingruntime
    template.openshift.io/long-description: This template defines resources needed
      to deploy vllm servingruntime with Red Hat Data Science KServe
      for LLM model
  labels:
    app: odh-dashboard
    app.kubernetes.io/part-of: rhods-dashboard
    app.opendatahub.io/rhods-dashboard: "true"
    opendatahub.io/dashboard: "true"
    opendatahub.io/ootb: "true"
  name: vllm-serving-template
  namespace: redhat-ods-applications
objects:
- apiVersion: serving.kserve.io/v1alpha1
  kind: ServingRuntime
  metadata:
    labels:
      opendatahub.io/dashboard: "true"
    annotations:
      openshift.io/display-name: vLLM
    name: vllm
  spec:
    builtInAdapter:
      modelLoadingTimeoutMillis: 90000
    containers:
      - args:
          - --model
          - /mnt/models/
          - --download-dir
          - /models-cache
          - --port
          - "8080"
          - --max-model-len
          - "6144"
        image: quay.io/rh-aiservices-bu/vllm-openai-ubi9:0.4.0.post1
        name: kserve-container
        ports:
          - containerPort: 8080
            name: http1
            protocol: TCP
    multiModel: false
    supportedModelFormats:
      - autoSelect: true
        name: pytorch